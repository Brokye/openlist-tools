import requests
from bs4 import BeautifulSoup
import csv
import time
import re
import json
import random
import os
import unicodedata
from difflib import SequenceMatcher

# =================é…ç½®éƒ¨åˆ†=================
INPUT_FILE = 'd_code.txt'
OUTPUT_FILE = 'result.csv'
MIN_SIMILARITY = 0.65  # å»ºè®®é˜ˆå€¼æé«˜åˆ° 0.65 (å› ä¸ºæ¸…æ´—ååŒ¹é…åº¦ä¼šå˜é«˜)

HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
    'Accept-Language': 'ja,en-US;q=0.9,en;q=0.8',
    'Referer': 'https://www.dmm.co.jp/'
}
DMM_COOKIES = {'age_check_done': '1', 'ckcy': '1'}


# ==========================================

class TitleMatcher:
    """
    ä¸“é—¨ç”¨äºå¤„ç†æ—¥æ–‡åŒäººéŸ³å£°æ ‡é¢˜åŒ¹é…çš„å·¥å…·ç±»
    """

    def __init__(self):
        # 1. æ ‡ç­¾æ­£åˆ™ï¼šåŒ¹é… ã€...ã€‘ æˆ– [...]
        self.pattern_tags = re.compile(r'ã€.*?ã€‘|\[.*?\]|\(.*?\)')
        # 2. å™ªå£°æ­£åˆ™ï¼šåŒ¹é…éæ–‡å­—ç¬¦å·
        self.pattern_noise = re.compile(r'[\sã€€~ï½\-\:ï¼šÃ—\.â€¦!ï¼\?ï¼Ÿâ—‹â—â—â˜…â˜†â—†â—‡â– â–¡â–³â–²â–½â–¼â€»ï¼Š*]')

    def normalize(self, text):
        """æ ¸å¿ƒæ¸…æ´—é€»è¾‘"""
        if not text: return ""

        # Step 1: NFKC æ ‡å‡†åŒ– (å…¨è§’è½¬åŠè§’)
        text = unicodedata.normalize('NFKC', text)

        # Step 2: ç§»é™¤æ ‡ç­¾ (ã€è€³ã‹ãã€‘ç­‰)
        text = self.pattern_tags.sub('', text)

        # Step 3: ç§»é™¤é€šç”¨å™ªå£°ç¬¦å·
        text = self.pattern_noise.sub('', text)

        # Step 4: æ—¥è¯­å¼‚å½¢è¯ä¿®æ­£ (å…³é”®!)
        # å°† DMM ä¹ æƒ¯çš„ "ç™’ã‚„ã—" ç»Ÿä¸€ä¸º DLsite ä¹ æƒ¯çš„ "ç™’ã—"
        text = text.replace('ç™’ã‚„ã—', 'ç™’ã—')

        # Step 5: ç§»é™¤åŠ©è¯ (å¯é€‰ï¼Œå‡å°‘è¯­æ³•å·®å¼‚)
        text = re.sub(r'[ã‚’ãŒã®]', '', text)

        return text.lower()

    def get_similarity(self, str1, str2):
        """è®¡ç®—æ¸…æ´—åçš„ç›¸ä¼¼åº¦"""
        norm1 = self.normalize(str1)
        norm2 = self.normalize(str2)
        if not norm1 or not norm2: return 0.0
        return SequenceMatcher(None, norm1, norm2).ratio()


def get_dmm_title(d_code):
    """ä» DMM è·å–æ ‡é¢˜"""
    # æ³¨æ„ï¼šè¿™é‡Œä½¿ç”¨çš„æ˜¯æœç´¢é¡µï¼Œä¸ºäº†å‡†ç¡®æ€§ï¼Œå»ºè®®ç¡®è®¤ searchstr æ˜¯å¦åªè¿”å›å”¯ä¸€ç»“æœ
    url = f"https://www.dmm.co.jp/search/=/searchstr={d_code}/limit=30/sort=rankprofile"
    try:
        response = requests.get(url, headers=HEADERS, cookies=DMM_COOKIES, timeout=15)
        if response.status_code != 200: return None
        soup = BeautifulSoup(response.text, 'html.parser')

        # å°è¯•é€‚é…ä¸¤ç§å¸¸è§çš„ DMM åˆ—è¡¨ç»“æ„
        title_tag = soup.find('p', class_="text-sm font-bold line-clamp-2")  # ç°ä»£æ ·å¼
        if not title_tag:
            # å¤‡ç”¨é€‰æ‹©å™¨ (åˆ—è¡¨æ ·å¼)
            title_tag = soup.find('span', class_="txt")

        if title_tag:
            return title_tag.get_text(strip=True)
        return None
    except Exception as e:
        print(f"DMM è¯·æ±‚é”™è¯¯: {e}")
        return None


def generate_search_candidates(raw_title):
    """
    ç”Ÿæˆæœç´¢å…³é”®è¯åˆ—è¡¨ã€‚
    ç­–ç•¥ï¼š
    1. æ¸…æ´—åçš„å…¨å (æœ€å‡†)
    2. ç§»é™¤ç‰¹å®šä¼å­—åçš„åç§°
    """
    candidates = []

    # åŸºç¡€æ¸…æ´—ï¼šç§»é™¤å¼€å¤´ç»“å°¾çš„ç©ºæ ¼
    base_title = raw_title.strip()

    # ç­–ç•¥ 1: ä¿®å¤å¸¸è§çš„ä¼å­— (DMM ç»å¸¸æŠŠ 'å¥´éš·' å†™æˆ 'å¥´â—')
    fixed_title = base_title
    fixed_title = re.sub(r'å¥´[â—â—‹]', 'å¥´éš·', fixed_title)
    fixed_title = re.sub(r'èª¿[â—â—‹]', 'èª¿æ•™', fixed_title)
    fixed_title = re.sub(r'ãƒ¬[â—â—‹Ã—]ãƒ—', 'ãƒ¬ã‚¤ãƒ—', fixed_title)

    # ç§»é™¤å¸¸è§çš„å¹²æ‰°ç¬¦å·ï¼Œç”Ÿæˆçº¯å‡€æ ‡é¢˜ä½œä¸ºæœç´¢è¯
    clean_search = re.sub(r'[â—‹â—â—â˜…â˜†â—†â—‡â– â–¡â–³â–²â–½â–¼â€»Ã—ï¼Š*]', ' ', fixed_title)
    clean_search = re.sub(r'\s+', ' ', clean_search).strip()  # åˆå¹¶ç©ºæ ¼

    candidates.append(clean_search)

    # ç­–ç•¥ 2: å¦‚æœæ ‡é¢˜éå¸¸é•¿ï¼Œå°è¯•æˆªå–å‰åŠéƒ¨åˆ† (DLsite æœç´¢æœ‰æ—¶å€™å¯¹è¿‡é•¿å…³é”®è¯æ”¯æŒä¸å¥½)
    # æˆªå–ç›´åˆ°é‡åˆ°ç¬¬ä¸€ä¸ªç‰¹æ®Šç¬¦å·æˆ–ç©ºæ ¼ï¼Œé•¿åº¦è‡³å°‘è¦ 5
    if len(clean_search) > 10:
        short_search = clean_search[:15]
        if short_search not in candidates:
            candidates.append(short_search)

    return candidates


def get_dlsite_candidates_list(search_term):
    """è°ƒç”¨ DLsite Suggest API è·å–å€™é€‰åˆ—è¡¨"""
    if len(search_term) < 2: return []

    base_url = "https://www.dlsite.com/suggest/?"
    timestamp = int(time.time() * 1000)
    callback_name = f"jQuery{random.randint(10 ** 19, 10 ** 20 - 1)}_{timestamp}"
    params = {
        'callback': callback_name,
        'term': search_term,
        'site': 'adult-jp',
        'time': timestamp,
        '_': timestamp + 5
    }

    try:
        response = requests.get(base_url, params=params, headers=HEADERS, timeout=10)
        if response.status_code == 200:
            match = re.search(r'^\s*.*?\(({.*})\);\s*$', response.text, re.DOTALL)
            if match:
                data = json.loads(match.group(1))
                return data.get('work', [])
    except Exception as e:
        print(f"DLsite API é”™è¯¯: {e}")
    return []


def main():
    if not os.path.exists(INPUT_FILE):
        print(f"âŒ é”™è¯¯: æ‰¾ä¸åˆ° {INPUT_FILE}")
        return

    # åˆå§‹åŒ–åŒ¹é…å™¨
    matcher = TitleMatcher()

    with open(INPUT_FILE, 'r', encoding='utf-8') as f:
        d_codes = [line.strip() for line in f if line.strip()]

    print(f"ğŸš€ å¼€å§‹å¤„ç† {len(d_codes)} ä¸ªæ¡ç›® (é›†æˆæ™ºèƒ½æ¸…æ´—ç‰ˆ)...")

    with open(OUTPUT_FILE, 'w', encoding='utf-8-sig', newline='') as csvfile:
        writer = csv.writer(csvfile)
        writer.writerow(['DMMåŸå', 'DLSiteåŒ¹é…æ ‡é¢˜', 'd_code', 'RJ_code', 'ç›¸ä¼¼åº¦', 'çŠ¶æ€'])

        for idx, d_code in enumerate(d_codes):
            print(f"\n[{idx + 1}/{len(d_codes)}] æ­£åœ¨æœç´¢: {d_code}")

            dmm_title = get_dmm_title(d_code)

            # å…¨å±€æœ€ä½³ç»“æœå®¹å™¨
            best_match = {
                "rj": "Not Found",
                "title": "",
                "score": 0.0,
                "status": "æœªæ‰¾åˆ°"
            }

            if dmm_title:
                print(f"    ğŸ“ DMMæ ‡é¢˜: {dmm_title[:40]}...")

                search_terms = generate_search_candidates(dmm_title)

                # å·²æ£€æŸ¥è¿‡çš„ RJ å·é›†åˆï¼Œé¿å…é‡å¤è®¡ç®—
                checked_rjs = set()

                for term in search_terms:
                    # å¦‚æœå·²ç»æ‰¾åˆ°äº†æé«˜ç›¸ä¼¼åº¦ (>0.9)ï¼Œè·³è¿‡åç»­æœç´¢è¯
                    if best_match["score"] > 0.9:
                        break

                    candidates_list = get_dlsite_candidates_list(term)

                    if candidates_list:
                        # éå†è¯¥æœç´¢è¯è¿”å›çš„æ‰€æœ‰ç»“æœ
                        for item in candidates_list:
                            dl_rj = item.get('workno')
                            dl_title = item.get('work_name')

                            if dl_rj in checked_rjs: continue
                            checked_rjs.add(dl_rj)

                            # === æ ¸å¿ƒï¼šä½¿ç”¨æ¸…æ´—åçš„ç›¸ä¼¼åº¦è®¡ç®— ===
                            sim = matcher.get_similarity(dmm_title, dl_title)

                            # è°ƒè¯•æ—¥å¿— (å¯é€‰)
                            # if sim > 0.5:
                            #     print(f"       å€™é€‰: {dl_rj} | åˆ†æ•°: {sim:.2f} | {dl_title[:15]}...")

                            if sim > best_match["score"]:
                                best_match["score"] = sim
                                best_match["rj"] = dl_rj
                                best_match["title"] = dl_title
                                best_match["status"] = "æˆåŠŸ"

                    time.sleep(random.uniform(0.5, 1.0))  # éšæœºå»¶è¿Ÿ

                # æœ€ç»ˆåˆ¤å®š
                if best_match["rj"] != "Not Found":
                    print(
                        f"    âœ… æœ€ç»ˆé€‰ä¸­: {best_match['rj']} | ç›¸ä¼¼åº¦: {best_match['score']:.2f} | {best_match['title'][:20]}...")

                    if best_match["score"] < MIN_SIMILARITY:
                        best_match["status"] = "ç›¸ä¼¼åº¦è¿‡ä½"
                        print(f"    âš ï¸ è­¦å‘Š: ç›¸ä¼¼åº¦ä½äºé˜ˆå€¼ ({MIN_SIMILARITY})")
                else:
                    print("    âŒ æœªæ‰¾åˆ°ä»»ä½•åŒ¹é…")

            else:
                print("    âš ï¸ DMMæ ‡é¢˜è·å–å¤±è´¥")
                best_match["status"] = "DMM Error"
                dmm_title = "Error"

            # å†™å…¥ CSV
            writer.writerow([
                dmm_title,
                best_match["title"],
                d_code,
                best_match["rj"],
                f"{best_match['score']:.2f}",
                best_match["status"]
            ])

            # è¿™é‡Œçš„ sleep æ˜¯ä¸ºäº†é˜²æ­¢ DMM å° IP
            time.sleep(random.uniform(1.0, 2.0))

    print(f"\nğŸ‰ å¤„ç†å®Œæˆï¼Œç»“æœå·²ä¿å­˜è‡³ {OUTPUT_FILE}")


if __name__ == "__main__":
    main()
